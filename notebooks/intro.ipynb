{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79607d54-8925-4ed6-8578-45a24df4c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.pgn\n",
    "import io\n",
    "from typing import List\n",
    "from chess.pgn import Game\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src')  # Adjust as necessary based on your directory structure\n",
    "\n",
    "from load import load_pgn_chunk\n",
    "from data import ChessDataset\n",
    "from cnn import ChessCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e7733-83ff-412a-bb23-14374510fb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5feb3a42-d2de-4d53-9b34-42380ade8896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 games in the first chunk\n"
     ]
    }
   ],
   "source": [
    "# Define your file path and chunk size\n",
    "pgn_file_path = \"../dataset/lichess_db_standard_rated_2016-05.pgn\"\n",
    "chunk_size = 10_000  # for example, load the first 100 games\n",
    "\n",
    "# Load the first chunk\n",
    "first_chunk = load_pgn_chunk(pgn_file_path, chunk_size=chunk_size, chunk_number=1)\n",
    "print(f\"Loaded {len(first_chunk)} games in the first chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01acdb0d-6054-49e5-a626-550ea53066ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_moves = []\n",
    "\n",
    "for game in first_chunk:\n",
    "    moves = [move.uci() for move in game.mainline_moves()]\n",
    "    all_moves.append(moves)\n",
    "\n",
    "size_in_bytes = sys.getsizeof(all_moves)  # Size of the list structure itself\n",
    "size_in_bytes += sum(sys.getsizeof(move) for move in all_moves)  # Adding the size of each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f5627c-332a-4c61-81de-59bfb6626c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size in bytes: 6493016\n",
      "Total size in megabytes: 6.192222595214844\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total size in bytes: {size_in_bytes}\")\n",
    "print(f\"Total size in megabytes: {size_in_bytes / (1024 * 1024)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51063190-dd65-4010-9e44-25315baf8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for game in first_chunk:\n",
    "#    print(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f5e10-9c63-4c66-bf2f-8be32251daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# For a list of integers\n",
    "size_in_bytes = sys.getsizeof(first_chunk)  # Size of the list structure itself\n",
    "size_in_bytes += sum(sys.getsizeof(item) for item in first_chunk)  # Adding the size of each item\n",
    "\n",
    "print(f\"Total size in bytes: {size_in_bytes}\")\n",
    "print(f\"Total size in megabytes: {size_in_bytes / (1024 * 1024)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0212d7e-1f39-46a8-b4d9-f808fa4fd610",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a995599-f6c7-4131-b37c-f714c4c61d5c",
   "metadata": {},
   "source": [
    "In building the supervised learning part of the model, we'll focus on training a network to predict the next move in a given chess position.\n",
    "\n",
    "We will therefore process the PGN dataset to extract board positions and corresponding moves. This will involve converting chessboard positions into a suitable numerical format for the CNN.\n",
    "\n",
    "We will design a CNN that takes the board position as input. The output layer should predict the probability of each possible move. Since there are a limited number of legal moves in any given position, this becomes a multi-class classification problem.\n",
    "\n",
    "### The Input Layer\n",
    "\n",
    "The input layer should reflect the board layout (8x8 squares with channels representing different pieces).\n",
    "\n",
    "### The Output Layer\n",
    "\n",
    "The output should represent all possible moves. In chess, a common approach is to have a fixed-size array where each entry corresponds to a potential move."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066afea-2a9f-4c0a-9323-4dda836cae1e",
   "metadata": {},
   "source": [
    "Our first mission is to construct the input and output tensor:\n",
    "X --> Y\n",
    "\n",
    "Where X, the input is the board layout at every single game state and Y is the corresponding next move (output).\n",
    "\n",
    "This is an example of the data for a given game:\n",
    "\n",
    "```Moves: e2e4 d7d5 e4d5 d8d5 b1c3 d5d8 d2d4 g8f6 g1f3 c8g4 h2h3 g4f3 g2f3 c7c6 f1g2 b8d7 c1e3 e7e6 d1d2 f6d5 c3d5 c6d5 e1c1 f8e7 c2c3 d8c7 c1b1 e8c8 f3f4 c8b8 h1g1 b8a8 g2h1 g7g6 h3h4 e7h4 f2f3 h4e7 d2c2 d7f6 h1g2 f6h5 g2h3 h5f4 e3f4 c7f4 d1f1 f4d6 g1g4 d8f8 f1g1 f7f5 g4g2 e7f6 g2g3 f8g8 h3f1 g8g7 f1d3 h8g8 c2h2 d6b8 h2g2 b8c8 f3f4 c8c6 g2f2 f6h4 g3g6 h4f2 g6g7 g8g7 g1g7 a7a6 g7g8 a8a7 g8h8 c6d7 h8h7 d7h7```\n",
    "\n",
    "To construct the input we will have to recreate the board layout for every single game state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94871b7-08de-40cf-9ccd-4b212703ca0f",
   "metadata": {},
   "source": [
    "### X Tensor (Board State Representation)\n",
    "The X tensor represents the state of the chess board.\n",
    "\n",
    "X will be a 3-dimensional tensor, 2-dimensions representing the layout (8x8) and a third dimension represnting the type of chess piece discriminated by color. The tensor can be represented as follows:\n",
    "\n",
    "$X \\in \\mathbb{R}^{8 \\times 8 \\times 12}$\n",
    "\n",
    "Each element $X_{i,j,k}$ of this tensor can be defined as:\n",
    "\n",
    "$$\\ X_{i,j,k} = \n",
    "   \\begin{cases} \n",
    "   1 & \\text{if piece type } k \\text{ is present at position } (i, j) \\\\\n",
    "   0 & \\text{otherwise}\n",
    "   \\end{cases}\n",
    "\\$$\n",
    "\n",
    "So in essence we're dealing with a 3-dimensional tensor with binary states.\n",
    "\n",
    "### Y Tensor (Next Move Representation)\n",
    "\n",
    "There are 64 squares on a chessboard, so there are 64 possible starting points and 64 possible ending points for each move, leading to 64Ã—64=4096 possible moves (including illegal ones, which the model should learn to never predict).\n",
    "\n",
    "The Y tensor represents the next move for each given game state. We encode each move as a one-hot vector of length 64x64 (representing all possible source and destination squares), the tensor can be represented as:\n",
    "\n",
    "$Y \\in \\{0, 1\\}^{4096}$\n",
    "\n",
    "Each element $Y_{l}$ of this tensor, where $l$ corresponds to a combination of source and destination squares, can be defined as:\n",
    "\n",
    "$$Y_{l} = \n",
    "   \\begin{cases} \n",
    "   1 & \\text{if the move corresponds to the index } l \\\\\n",
    "   0 & \\text{otherwise}\n",
    "   \\end{cases}\n",
    "\\$$\n",
    "\n",
    "In this representation, the index $l$ is calculated based on the source square and the destination square of the move. For instance, if you flatten the 8x8 board into a 64-element array, then a move from square $a$ to square $b$ would correspond to an index $l = 64 \\times a + b$.\n",
    "\n",
    "### Summary\n",
    "- **X Tensor**: Represents the board state with a 3D tensor where the dimensions are board height, board width, and number of piece types.\n",
    "- **Y Tensor**: Represents the move as a one-hot encoded vector in a flattened 2D space of source and destination squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51494be8-5141-4f36-b4f9-76c8175c4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "game_moves = []\n",
    "\n",
    "for game in first_chunk:\n",
    "    moves = [move.uci() for move in game.mainline_moves()]\n",
    "\n",
    "    game_moves.append(moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666bcb07-db30-41d3-addc-33c979c62be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChessDataset(game_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c02e7a-7a13-459a-95be-8be7324bf9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checksum\n",
    "#for move in dataset.moves:\n",
    "#    assert(sum(move) == 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c04fcf-f171-4520-9b65-0cdb51d4c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.check_layout(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd34f8-f941-400d-9a9c-1ca6b909e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessCNN, self).__init__()\n",
    "\n",
    "        # We do not add padding. This approach respects the inherent structure of the\n",
    "        # chessboard and avoids introducing artificial concepts that are not present in\n",
    "        # the actual game. It allows the model to focus on learning meaningful spatial\n",
    "        # relationships and patterns that are genuinely indicative of chess strategies.\n",
    "        #\n",
    "        # Since we do not add padding, our spatial dimensions will be squashed to\n",
    "        # a 7x7 versus the original 8x8 layout\n",
    "        self.conv1 = nn.Conv2d(12, 64, kernel_size=2, padding=0)\n",
    "\n",
    "        # In the second layer we have a bigger kernel size to capture higher-level patterns\n",
    "        # and add padding of 1 to keep the spatial resolution from being further decreased\n",
    "        self.conv2 = nn.Conv2d(64, 124, kernel_size = 3, padding=1)\n",
    "\n",
    "        # We now move to the fully connected layer. We do not apply a pooling because there is no\n",
    "        # need for downsampling as our tensors are very reasonable in size\n",
    "        # Our tensor currently has the following shape: 7x7x124\n",
    "        #\n",
    "        # So we will need to flatten the tensor into one dimensional tensor with a size of: 7 * 7 * 124 = 6076\n",
    "        # This hidden layer will output 2048 neurons. We will afterwards test this number to balance it against under and overfitting\n",
    "        self.fc1 = nn.Linear(7 * 7 * 124, 2048)\n",
    "\n",
    "        self.fc2 = nn.Linear(2048, 4096)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Convolutional Layer 1\n",
    "        output1 = F.relu(self.conv1(input))\n",
    "        # Convolutional Layer 2\n",
    "        output2 = F.relu(self.conv2(output1))\n",
    "        # Flatten Layer\n",
    "        # We now flatten the layer from (124, 7, 7) to: (7 * 7 * 124 = 6076)\n",
    "        output3 = output2.view(7202, -1)  # -1 tells PyTorch to infer the correct size --> torch.Size([7202, 6076])\n",
    "        \n",
    "        output4 = F.relu(self.fc1(output3))\n",
    "        output5 = self.fc2(output4)\n",
    "        return F.softmax(output5, dim=1) # # torch.Size([7202, 4096])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391bae73-dfb4-400b-abee-fdaa6ea914a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb3ebf-7f96-4605-91bc-a936e877882f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.moves.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7eca8-8140-434d-9845-b61078eb652d",
   "metadata": {},
   "source": [
    "To run your input dataset through the first convolutional layer in PyTorch, we need to make sure that the input tensor is in the correct shape expected by `nn.Conv2d`. The `nn.Conv2d` layer expects the input tensor to have the shape \\([N, C, H, W]\\), where:\n",
    "\n",
    "- \\( N \\) is the batch size,\n",
    "- \\( C \\) is the number of channels,\n",
    "- \\( H \\) is the height of the image (or in your case, the chessboard),\n",
    "- \\( W \\) is the width of the image (or the chessboard).\n",
    "\n",
    "Given your dataset shape \\((7202, 8, 8, 12)\\), it seems you have the channels as the last dimension, but they should be the second dimension for `nn.Conv2d`. You will need to rearrange the dimensions of your input tensor to \\((N, C, H, W)\\), which in your case would be \\((7202, 12, 8, 8)\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ce4cc-8e4f-4257-bf6d-1bc8baf3f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define your optimizer, for example, using Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # lr is the learning rate\n",
    "\n",
    "\n",
    "# Trainning Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:  # Assuming you have a DataLoader for your data\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdba05a-a41f-478b-be4a-6f9b9bd9f316",
   "metadata": {},
   "source": [
    "In this training loop:\n",
    "\n",
    "optimizer.zero_grad() clears old gradients from the last step (otherwise they'll be accumulated).\n",
    "loss.backward() computes the gradient of the loss w.r.t the parameters (weights) of the model.\n",
    "optimizer.step() updates the weights.\n",
    "The learning rate is a critical hyperparameter in training neural networks and needs to be chosen carefully. If it's too high, the model might overshoot the optimal solution. If it's too low, training can be slow, or the model might get stuck in a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492bb1f-8a42-4dd8-b8b6-90d6cd52fc18",
   "metadata": {},
   "source": [
    "# As we get a sense that the loss is starting to plateau off\n",
    "# we can apply a learning decay\n",
    "lr = 0.01\n",
    "\n",
    "for i in range(10_000):\n",
    "    # Minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[X[ix]] # (32, 3, 2)The input matrix X using the vector embeddings\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # (32, 100) 32 inputs, 100 neurons\n",
    "    logits = h @ W2 + b2 # (32, 27) 32 inputs, 27 possible character outcomes\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    \n",
    "    # Backward pass\n",
    "    \n",
    "    # Just making sure the gradients are initialized to zero\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # This will compute all the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # parameter update\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "# However now this is the loss for the minibatch ONLY! It's not the global loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6646927-6b2f-41fc-b603-60206f8bbef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
